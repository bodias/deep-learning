{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 50:\n",
      "Image - Min Value: 23 Max Value: 244\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e1ef75ef0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 50\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculating min-max normalization\n",
    "    # x' = ( x - x_min ) / ( x_max - x_min)\n",
    "    \n",
    "    x_normalized = list()\n",
    "    x_min = np.min(x) # x_min\n",
    "    x_max = np.max(x) # x_max\n",
    "    \n",
    "    for i in x:\n",
    "        x_normalized.append( ( i - x_min ) / ( x_max - x_min ) )\n",
    "    \n",
    "    return np.array(x_normalized)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "map_one_hot = {}\n",
    "def one_hot_encode(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "\n",
    "    oneHot = OneHotEncoder(n_values=10)\n",
    "    \n",
    "    labels = np.array(x).reshape(len(x),1)\n",
    "    oneHot.fit(labels)\n",
    "    y = oneHot.transform(labels).toarray()\n",
    "    return y\n",
    "\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ph = tf.placeholder(shape=(None,image_shape[0],image_shape[1],image_shape[2]),name='x',dtype=tf.float32)\n",
    "    return ph\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ph = tf.placeholder(shape=(None,n_classes),name='y',dtype=tf.float32)\n",
    "    return ph\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ph = tf.placeholder(name='keep_prob',dtype=tf.float32)\n",
    "    return ph\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor.\n",
    "    weight = tf.Variable(tf.random_normal(\n",
    "                [conv_ksize[0], conv_ksize[1], int(x_tensor.shape[3]), conv_num_outputs], stddev=0.1))\n",
    "        \n",
    "    bias = tf.Variable(tf.random_normal([conv_num_outputs], stddev=0.1))\n",
    "    \n",
    "    # filter: A Tensor. Must have the same type as input. A 4-D tensor of \n",
    "    # shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    \n",
    "    # strides: A list of ints. 1-D tensor of length 4. The stride of \n",
    "    # the sliding window for each dimension of input. \n",
    "    # The dimension order is determined by the value of data_format, see below for details.\n",
    "    conv_layer = tf.nn.conv2d(x_tensor,\n",
    "                             filter=weight,\n",
    "                             strides=[1,conv_strides[0],conv_strides[1],1],\n",
    "                             padding='SAME',\n",
    "                             name='conv_1l')\n",
    "    \n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias, name='bias_2l')    \n",
    "    \n",
    "    activation = tf.nn.relu(conv_layer,name='act_3l')\n",
    "    \n",
    "    maxpool = tf.nn.max_pool(activation,\n",
    "                             ksize=[1,pool_ksize[0],pool_ksize[1],1],\n",
    "                             strides=[1,pool_strides[0],pool_strides[1],1],\n",
    "                             padding='SAME',\n",
    "                             name= 'maxp_4l')\n",
    "    return maxpool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs,activation_fn=tf.nn.relu)    \n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs,activation_fn=None)\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters\n",
    "    \n",
    "    conv_ksize = (8, 8)\n",
    "    conv_strides = (conv_ksize[0]/2, conv_ksize[1]/2)\n",
    "    conv_num_outputs = conv_ksize[0] * conv_ksize[1]\n",
    "    pool_ksize = (conv_strides[0], conv_strides[1])\n",
    "    pool_strides = (pool_ksize[0]/2, pool_ksize[1]/2)\n",
    "    num_outputs = 10\n",
    "\n",
    "    # Network\n",
    "    \n",
    "    conv_l1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv_l2 = conv2d_maxpool(conv_l1, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv_l3 = conv2d_maxpool(conv_l2, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    flat = flatten(conv_l3)\n",
    "    flat = tf.nn.dropout(flat, keep_prob)\n",
    "        \n",
    "    fully_conn_layer = fully_conn(flat, 100)\n",
    "    fully_conn_layer = tf.nn.dropout(fully_conn_layer, keep_prob)\n",
    "    \n",
    "    output_layer = output(fully_conn_layer, num_outputs)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    \n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                loss,\n",
    "                100* valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "keep_probability = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1891 Validation Accuracy: 19.819999\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.9737 Validation Accuracy: 26.560000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.8525 Validation Accuracy: 32.799995\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.7531 Validation Accuracy: 35.839996\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.6899 Validation Accuracy: 38.099998\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.6440 Validation Accuracy: 39.259994\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.5866 Validation Accuracy: 40.799996\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.5444 Validation Accuracy: 41.859999\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.4895 Validation Accuracy: 43.099999\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.4466 Validation Accuracy: 44.800001\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.4137 Validation Accuracy: 44.799998\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.4019 Validation Accuracy: 44.639999\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.3399 Validation Accuracy: 46.019995\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.3185 Validation Accuracy: 46.459994\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.2969 Validation Accuracy: 47.099996\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.2555 Validation Accuracy: 47.699994\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.2418 Validation Accuracy: 46.939999\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.1996 Validation Accuracy: 48.939997\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.1777 Validation Accuracy: 48.499995\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.1589 Validation Accuracy: 48.799998\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.1313 Validation Accuracy: 48.899993\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.0940 Validation Accuracy: 49.679995\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.0571 Validation Accuracy: 50.159997\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.0492 Validation Accuracy: 50.279999\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.0256 Validation Accuracy: 50.139993\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.0062 Validation Accuracy: 49.419999\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.9583 Validation Accuracy: 50.579995\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.9313 Validation Accuracy: 50.839990\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.9104 Validation Accuracy: 50.659990\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.9038 Validation Accuracy: 50.919998\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.8805 Validation Accuracy: 51.259995\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.8749 Validation Accuracy: 50.579995\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.9029 Validation Accuracy: 50.319999\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.8493 Validation Accuracy: 51.259995\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.8962 Validation Accuracy: 48.719996\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.8815 Validation Accuracy: 49.639994\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.8429 Validation Accuracy: 51.319993\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.7634 Validation Accuracy: 52.339995\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.7625 Validation Accuracy: 51.759994\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.7298 Validation Accuracy: 51.879996\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.7121 Validation Accuracy: 52.239996\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.7163 Validation Accuracy: 51.559997\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.7403 Validation Accuracy: 50.499994\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.7393 Validation Accuracy: 50.839996\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.7022 Validation Accuracy: 51.039994\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.7442 Validation Accuracy: 49.819994\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.7472 Validation Accuracy: 50.359994\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.6945 Validation Accuracy: 52.079999\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.6390 Validation Accuracy: 52.559996\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.6870 Validation Accuracy: 51.079994\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.6100 Validation Accuracy: 52.579999\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.6037 Validation Accuracy: 52.379996\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.5958 Validation Accuracy: 52.599996\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.6004 Validation Accuracy: 51.739991\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.5656 Validation Accuracy: 53.079987\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.5909 Validation Accuracy: 51.779991\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.5450 Validation Accuracy: 52.579993\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.5182 Validation Accuracy: 52.399993\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.5459 Validation Accuracy: 52.019995\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.5255 Validation Accuracy: 52.019989\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.4995 Validation Accuracy: 52.379996\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.5182 Validation Accuracy: 51.679999\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.4765 Validation Accuracy: 52.399999\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.4803 Validation Accuracy: 52.819997\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.4878 Validation Accuracy: 52.299994\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.5435 Validation Accuracy: 50.739992\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.5440 Validation Accuracy: 50.039995\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.5014 Validation Accuracy: 50.779998\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.4566 Validation Accuracy: 50.899994\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.4424 Validation Accuracy: 51.399994\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.5051 Validation Accuracy: 50.359994\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.4662 Validation Accuracy: 52.139992\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.4448 Validation Accuracy: 52.559996\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.4342 Validation Accuracy: 52.459997\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.4563 Validation Accuracy: 51.479995\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.4028 Validation Accuracy: 51.839995\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.4227 Validation Accuracy: 51.179993\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.4599 Validation Accuracy: 51.639998\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.4067 Validation Accuracy: 52.179992\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.4128 Validation Accuracy: 51.919991\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.5172 Validation Accuracy: 49.320000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.4759 Validation Accuracy: 50.659990\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.4540 Validation Accuracy: 52.119994\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.4396 Validation Accuracy: 51.099998\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.4329 Validation Accuracy: 51.599997\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.4108 Validation Accuracy: 51.099998\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.3958 Validation Accuracy: 51.459992\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.3924 Validation Accuracy: 51.779991\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.4919 Validation Accuracy: 49.119997\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.3877 Validation Accuracy: 51.059997\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.3704 Validation Accuracy: 51.239991\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.3707 Validation Accuracy: 51.779997\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.3800 Validation Accuracy: 51.099998\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.3604 Validation Accuracy: 51.259995\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.3393 Validation Accuracy: 51.319993\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.3189 Validation Accuracy: 52.319992\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.3574 Validation Accuracy: 51.439995\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.3278 Validation Accuracy: 51.719987\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.3352 Validation Accuracy: 51.299995\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.3314 Validation Accuracy: 51.419997\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.3753 Validation Accuracy: 50.339997\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.3924 Validation Accuracy: 49.360001\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.4222 Validation Accuracy: 49.519995\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.3541 Validation Accuracy: 51.019996\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.4607 Validation Accuracy: 49.119997\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.4625 Validation Accuracy: 49.539995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.3585 Validation Accuracy: 51.399994\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.3205 Validation Accuracy: 51.039994\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.3738 Validation Accuracy: 50.139999\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.3727 Validation Accuracy: 50.779998\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.3478 Validation Accuracy: 50.619996\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.3538 Validation Accuracy: 49.719995\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.2963 Validation Accuracy: 51.299995\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.2895 Validation Accuracy: 50.859994\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.2483 Validation Accuracy: 51.839995\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.2591 Validation Accuracy: 50.799996\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.2716 Validation Accuracy: 50.119996\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.2551 Validation Accuracy: 50.760001\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.2544 Validation Accuracy: 51.939994\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.3096 Validation Accuracy: 51.139992\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.2854 Validation Accuracy: 51.659989\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.2714 Validation Accuracy: 51.479995\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.2655 Validation Accuracy: 51.979995\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.2607 Validation Accuracy: 51.359993\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.2909 Validation Accuracy: 51.219994\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.2806 Validation Accuracy: 51.059997\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.2570 Validation Accuracy: 50.999999\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.2412 Validation Accuracy: 50.779992\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.2249 Validation Accuracy: 50.739992\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.2154 Validation Accuracy: 51.519996\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.2160 Validation Accuracy: 51.539999\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.2106 Validation Accuracy: 52.239990\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.2104 Validation Accuracy: 51.979995\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.1887 Validation Accuracy: 52.039993\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.1985 Validation Accuracy: 51.879996\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.2450 Validation Accuracy: 50.839996\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.1927 Validation Accuracy: 51.459992\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.1834 Validation Accuracy: 51.559997\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.1956 Validation Accuracy: 51.219988\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.1675 Validation Accuracy: 51.679993\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.1460 Validation Accuracy: 52.719992\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.1578 Validation Accuracy: 51.419997\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.1487 Validation Accuracy: 51.879996\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.1853 Validation Accuracy: 50.959992\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.1548 Validation Accuracy: 51.999992\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.1519 Validation Accuracy: 50.939995\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.1709 Validation Accuracy: 50.359994\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.2103 Validation Accuracy: 49.499995\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.1988 Validation Accuracy: 49.699998\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.1459 Validation Accuracy: 51.959991\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.1492 Validation Accuracy: 51.679993\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.1499 Validation Accuracy: 51.359993\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.1496 Validation Accuracy: 51.779997\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.1502 Validation Accuracy: 51.399994\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.1448 Validation Accuracy: 51.319993\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.1227 Validation Accuracy: 51.859993\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.1403 Validation Accuracy: 51.299995\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.1394 Validation Accuracy: 50.959998\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.1702 Validation Accuracy: 49.800000\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.1981 Validation Accuracy: 50.159997\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.2156 Validation Accuracy: 49.719998\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.1974 Validation Accuracy: 50.260001\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.1918 Validation Accuracy: 48.039997\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.1937 Validation Accuracy: 48.739994\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.1952 Validation Accuracy: 48.599997\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.1303 Validation Accuracy: 50.119996\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.1628 Validation Accuracy: 48.379993\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.3029 Validation Accuracy: 45.940000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.2601 Validation Accuracy: 49.659997\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.1621 Validation Accuracy: 51.379997\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.1476 Validation Accuracy: 50.159997\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.1265 Validation Accuracy: 50.839990\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.1217 Validation Accuracy: 51.199996\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.1420 Validation Accuracy: 50.739992\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.1434 Validation Accuracy: 50.239992\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.1340 Validation Accuracy: 50.539994\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.1535 Validation Accuracy: 50.439990\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.1619 Validation Accuracy: 49.439994\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.1475 Validation Accuracy: 50.139993\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.1433 Validation Accuracy: 50.199991\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.1187 Validation Accuracy: 50.479996\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.1076 Validation Accuracy: 50.399995\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.1203 Validation Accuracy: 49.679998\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.1092 Validation Accuracy: 50.779998\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.1047 Validation Accuracy: 50.959992\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.0939 Validation Accuracy: 51.179993\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.1061 Validation Accuracy: 50.839996\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.1251 Validation Accuracy: 49.839991\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.1382 Validation Accuracy: 50.199997\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.1066 Validation Accuracy: 51.299995\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.1046 Validation Accuracy: 51.579988\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.1192 Validation Accuracy: 51.400000\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.0925 Validation Accuracy: 51.319993\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.0926 Validation Accuracy: 50.999993\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.0866 Validation Accuracy: 50.399995\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.0748 Validation Accuracy: 51.099998\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.0984 Validation Accuracy: 49.999994\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.0829 Validation Accuracy: 50.259995\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.1221 Validation Accuracy: 49.999994\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.0892 Validation Accuracy: 49.939993\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2428 Validation Accuracy: 18.820000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.0772 Validation Accuracy: 26.079997\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.9043 Validation Accuracy: 28.799999\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.7745 Validation Accuracy: 32.640001\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.7573 Validation Accuracy: 34.279999\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.7645 Validation Accuracy: 37.799999\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.6643 Validation Accuracy: 39.579996\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.4835 Validation Accuracy: 40.899998\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.4875 Validation Accuracy: 43.339992\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.5188 Validation Accuracy: 44.199997\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.5738 Validation Accuracy: 44.579995\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.5056 Validation Accuracy: 45.239997\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.3034 Validation Accuracy: 45.619997\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.3525 Validation Accuracy: 47.739998\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.3869 Validation Accuracy: 46.699995\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.4781 Validation Accuracy: 48.379996\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.4266 Validation Accuracy: 47.039998\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.2412 Validation Accuracy: 47.739995\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.2767 Validation Accuracy: 49.259996\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.3241 Validation Accuracy: 49.039999\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.4335 Validation Accuracy: 50.419992\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.3938 Validation Accuracy: 48.439991\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.1910 Validation Accuracy: 49.519998\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.2558 Validation Accuracy: 50.479990\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.2698 Validation Accuracy: 50.559998\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.3823 Validation Accuracy: 51.019990\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.3163 Validation Accuracy: 49.859995\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.1439 Validation Accuracy: 51.519990\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.1796 Validation Accuracy: 51.959991\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.2130 Validation Accuracy: 51.519996\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.3286 Validation Accuracy: 52.359992\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.2630 Validation Accuracy: 51.859993\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.0953 Validation Accuracy: 52.099997\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.1717 Validation Accuracy: 52.380002\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.1816 Validation Accuracy: 52.599996\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.3002 Validation Accuracy: 52.539992\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.2209 Validation Accuracy: 52.839994\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.0816 Validation Accuracy: 53.039992\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.1196 Validation Accuracy: 53.879994\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.1752 Validation Accuracy: 53.299993\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.2740 Validation Accuracy: 53.399992\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.1862 Validation Accuracy: 53.959996\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.0457 Validation Accuracy: 53.879994\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.1022 Validation Accuracy: 54.279995\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.1264 Validation Accuracy: 54.439992\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.2530 Validation Accuracy: 54.059994\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.1718 Validation Accuracy: 54.119992\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.0198 Validation Accuracy: 54.459995\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.0874 Validation Accuracy: 54.179990\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.1151 Validation Accuracy: 54.519993\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.2073 Validation Accuracy: 54.359996\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.1376 Validation Accuracy: 55.079985\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.0158 Validation Accuracy: 55.159992\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.0265 Validation Accuracy: 54.319996\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.0980 Validation Accuracy: 53.979993\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.2013 Validation Accuracy: 54.519993\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.1201 Validation Accuracy: 54.979992\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.0005 Validation Accuracy: 55.419993\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.0095 Validation Accuracy: 55.219996\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.0982 Validation Accuracy: 54.759991\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.1806 Validation Accuracy: 54.579991\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.1247 Validation Accuracy: 54.439992\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.9658 Validation Accuracy: 55.779994\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.0232 Validation Accuracy: 55.099994\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.0721 Validation Accuracy: 55.759996\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.1729 Validation Accuracy: 54.679990\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.0984 Validation Accuracy: 54.459989\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.9837 Validation Accuracy: 56.119990\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.9654 Validation Accuracy: 56.359988\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.0198 Validation Accuracy: 56.059992\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.1449 Validation Accuracy: 56.199992\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.0856 Validation Accuracy: 55.079991\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.9638 Validation Accuracy: 56.239998\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.9699 Validation Accuracy: 56.859994\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.9894 Validation Accuracy: 57.439989\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.1023 Validation Accuracy: 56.179994\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.0424 Validation Accuracy: 55.719995\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.9147 Validation Accuracy: 57.339996\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.9164 Validation Accuracy: 57.399994\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.9892 Validation Accuracy: 57.239991\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.0965 Validation Accuracy: 56.439990\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.0276 Validation Accuracy: 56.299990\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.9240 Validation Accuracy: 57.179987\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.9173 Validation Accuracy: 57.759994\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.9773 Validation Accuracy: 56.479990\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.0688 Validation Accuracy: 56.999993\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.0077 Validation Accuracy: 57.079995\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.9003 Validation Accuracy: 57.759988\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.9138 Validation Accuracy: 58.139992\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.9287 Validation Accuracy: 58.379996\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.0727 Validation Accuracy: 56.399995\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.9639 Validation Accuracy: 57.119995\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.8700 Validation Accuracy: 57.879996\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.8908 Validation Accuracy: 58.699989\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.9384 Validation Accuracy: 56.719995\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.0278 Validation Accuracy: 58.519995\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.9895 Validation Accuracy: 56.279993\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.8822 Validation Accuracy: 57.979995\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.8920 Validation Accuracy: 58.979988\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.9087 Validation Accuracy: 57.459986\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.0254 Validation Accuracy: 57.899988\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.9786 Validation Accuracy: 56.139994\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.8706 Validation Accuracy: 58.239990\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.8696 Validation Accuracy: 58.739990\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.8810 Validation Accuracy: 58.719993\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.0103 Validation Accuracy: 57.859987\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.9206 Validation Accuracy: 57.379997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.8522 Validation Accuracy: 57.859993\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.8518 Validation Accuracy: 59.359992\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.8688 Validation Accuracy: 58.959991\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.9923 Validation Accuracy: 57.879996\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.9140 Validation Accuracy: 57.719994\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.8805 Validation Accuracy: 57.719994\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.8494 Validation Accuracy: 59.019989\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.8617 Validation Accuracy: 59.299994\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.9936 Validation Accuracy: 57.419991\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.9366 Validation Accuracy: 56.279993\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.8775 Validation Accuracy: 57.819998\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.8514 Validation Accuracy: 58.699995\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.8656 Validation Accuracy: 59.919989\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.9737 Validation Accuracy: 58.879995\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.9524 Validation Accuracy: 56.079990\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.8313 Validation Accuracy: 59.419990\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.8296 Validation Accuracy: 59.200001\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.8537 Validation Accuracy: 59.839994\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.9817 Validation Accuracy: 59.619993\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.9456 Validation Accuracy: 56.559992\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.8071 Validation Accuracy: 59.219998\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.8139 Validation Accuracy: 58.899993\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.8774 Validation Accuracy: 58.919990\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.9836 Validation Accuracy: 59.219992\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.8996 Validation Accuracy: 57.819992\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.7870 Validation Accuracy: 59.779990\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.8139 Validation Accuracy: 59.639990\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.8441 Validation Accuracy: 59.979999\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.9655 Validation Accuracy: 59.679991\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.8636 Validation Accuracy: 57.459992\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.7762 Validation Accuracy: 59.919989\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.8058 Validation Accuracy: 59.139991\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.8183 Validation Accuracy: 59.879994\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.9581 Validation Accuracy: 58.999991\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.8636 Validation Accuracy: 58.219993\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.8043 Validation Accuracy: 60.339987\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.8133 Validation Accuracy: 59.679991\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.8030 Validation Accuracy: 60.239983\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.9455 Validation Accuracy: 59.039992\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.8212 Validation Accuracy: 59.579986\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.7957 Validation Accuracy: 60.320002\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.8345 Validation Accuracy: 58.079988\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.7765 Validation Accuracy: 60.459995\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.9646 Validation Accuracy: 58.619994\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.8513 Validation Accuracy: 58.219993\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.8029 Validation Accuracy: 59.179986\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.7839 Validation Accuracy: 59.139991\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.7709 Validation Accuracy: 60.559994\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.9295 Validation Accuracy: 59.479994\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.8269 Validation Accuracy: 59.159988\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.7933 Validation Accuracy: 59.659988\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.7791 Validation Accuracy: 59.939992\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.7555 Validation Accuracy: 60.639995\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.9039 Validation Accuracy: 59.979987\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.7943 Validation Accuracy: 59.779990\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.7594 Validation Accuracy: 60.139990\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.7528 Validation Accuracy: 59.779996\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.7461 Validation Accuracy: 60.819989\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.9157 Validation Accuracy: 59.319991\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.7949 Validation Accuracy: 59.539992\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.7634 Validation Accuracy: 60.239995\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.7505 Validation Accuracy: 59.819990\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.7309 Validation Accuracy: 60.839987\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.8969 Validation Accuracy: 59.759992\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.8030 Validation Accuracy: 59.519994\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.7496 Validation Accuracy: 59.939992\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.7181 Validation Accuracy: 60.219991\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.7334 Validation Accuracy: 60.719991\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.8854 Validation Accuracy: 60.099995\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.8086 Validation Accuracy: 58.979988\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.7239 Validation Accuracy: 60.939991\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.7266 Validation Accuracy: 60.139990\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.7087 Validation Accuracy: 60.839987\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.8882 Validation Accuracy: 59.559989\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.7692 Validation Accuracy: 60.239995\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.7106 Validation Accuracy: 61.099994\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.7106 Validation Accuracy: 60.639989\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.7106 Validation Accuracy: 60.399985\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.8750 Validation Accuracy: 60.899997\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.7716 Validation Accuracy: 60.119987\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.7129 Validation Accuracy: 60.899985\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.7015 Validation Accuracy: 60.319996\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.6957 Validation Accuracy: 60.839993\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.8795 Validation Accuracy: 60.519993\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.7707 Validation Accuracy: 59.939992\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.6881 Validation Accuracy: 61.619991\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.6953 Validation Accuracy: 60.399991\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.6835 Validation Accuracy: 60.699993\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.8564 Validation Accuracy: 60.639989\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.7716 Validation Accuracy: 59.499997\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.7047 Validation Accuracy: 61.319989\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.7082 Validation Accuracy: 60.259986\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.6791 Validation Accuracy: 61.299992\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.8654 Validation Accuracy: 60.539991\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.7424 Validation Accuracy: 61.079991\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.6856 Validation Accuracy: 61.419988\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.6846 Validation Accuracy: 61.099988\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.6749 Validation Accuracy: 60.859990\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.8563 Validation Accuracy: 60.939991\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.7566 Validation Accuracy: 60.279989\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.6651 Validation Accuracy: 61.799985\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.6923 Validation Accuracy: 60.839987\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.6626 Validation Accuracy: 61.519986\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.8488 Validation Accuracy: 60.519987\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.7465 Validation Accuracy: 60.339987\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.7099 Validation Accuracy: 61.039996\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.6936 Validation Accuracy: 61.299986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.6731 Validation Accuracy: 60.999990\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.8714 Validation Accuracy: 60.699987\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.7462 Validation Accuracy: 60.359991\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.7017 Validation Accuracy: 60.279995\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.6822 Validation Accuracy: 61.599994\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.6745 Validation Accuracy: 61.319995\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.8506 Validation Accuracy: 61.159992\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.7213 Validation Accuracy: 60.359991\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.6814 Validation Accuracy: 61.339986\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.6950 Validation Accuracy: 60.999990\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.6588 Validation Accuracy: 60.999995\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.8336 Validation Accuracy: 61.099994\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.7585 Validation Accuracy: 59.139997\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.6894 Validation Accuracy: 61.199987\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.6781 Validation Accuracy: 61.079991\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.6632 Validation Accuracy: 61.859989\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.8483 Validation Accuracy: 60.199988\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.7631 Validation Accuracy: 60.259992\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.6614 Validation Accuracy: 61.339992\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.6818 Validation Accuracy: 61.219996\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.6547 Validation Accuracy: 61.859989\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.8418 Validation Accuracy: 60.299993\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.7551 Validation Accuracy: 59.899992\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.6783 Validation Accuracy: 61.519992\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.6810 Validation Accuracy: 61.259991\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.6532 Validation Accuracy: 61.099994\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.8132 Validation Accuracy: 60.819995\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.7312 Validation Accuracy: 60.379994\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.7019 Validation Accuracy: 60.059994\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.6924 Validation Accuracy: 60.859990\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.6736 Validation Accuracy: 59.999990\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.8410 Validation Accuracy: 60.359991\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.7062 Validation Accuracy: 61.099988\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.6564 Validation Accuracy: 61.219996\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.6492 Validation Accuracy: 61.279994\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.6573 Validation Accuracy: 60.559994\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.7958 Validation Accuracy: 61.239988\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.6846 Validation Accuracy: 61.639988\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.6527 Validation Accuracy: 62.159991\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.6571 Validation Accuracy: 61.159992\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.6303 Validation Accuracy: 61.479992\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.8101 Validation Accuracy: 60.659993\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.6846 Validation Accuracy: 61.799991\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.6310 Validation Accuracy: 61.779988\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.6463 Validation Accuracy: 60.979986\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.6403 Validation Accuracy: 60.539991\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.8119 Validation Accuracy: 60.319990\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.6650 Validation Accuracy: 62.219995\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.6228 Validation Accuracy: 61.679989\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.6508 Validation Accuracy: 60.619986\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.6226 Validation Accuracy: 61.319989\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.7988 Validation Accuracy: 61.279988\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.6559 Validation Accuracy: 61.979991\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.6214 Validation Accuracy: 61.839992\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.6350 Validation Accuracy: 61.239988\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.6376 Validation Accuracy: 60.939991\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.7931 Validation Accuracy: 61.259985\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.6776 Validation Accuracy: 61.259991\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.6139 Validation Accuracy: 61.879987\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.6236 Validation Accuracy: 61.939991\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.6032 Validation Accuracy: 61.099988\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.7952 Validation Accuracy: 61.099994\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.6723 Validation Accuracy: 61.259991\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.6006 Validation Accuracy: 61.739993\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.6237 Validation Accuracy: 62.019986\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.6114 Validation Accuracy: 61.059994\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.7776 Validation Accuracy: 60.999990\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.6621 Validation Accuracy: 61.479992\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.5757 Validation Accuracy: 62.039989\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.6149 Validation Accuracy: 61.799991\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.5844 Validation Accuracy: 61.219990\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.7765 Validation Accuracy: 61.599988\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.6781 Validation Accuracy: 60.779995\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.5880 Validation Accuracy: 61.599994\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.6199 Validation Accuracy: 62.339985\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.6102 Validation Accuracy: 59.759992\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.7925 Validation Accuracy: 61.519992\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.6341 Validation Accuracy: 61.779994\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.6065 Validation Accuracy: 61.139989\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.6067 Validation Accuracy: 61.899990\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.5880 Validation Accuracy: 61.059994\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.7616 Validation Accuracy: 61.479992\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.6340 Validation Accuracy: 61.899984\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.5794 Validation Accuracy: 62.419987\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.6121 Validation Accuracy: 62.119991\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.5925 Validation Accuracy: 60.779995\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.7382 Validation Accuracy: 62.019986\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.6102 Validation Accuracy: 62.319994\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.5983 Validation Accuracy: 61.399990\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.5985 Validation Accuracy: 61.919987\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.6154 Validation Accuracy: 60.259986\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.7429 Validation Accuracy: 61.839992\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.6121 Validation Accuracy: 62.459993\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.5803 Validation Accuracy: 61.379987\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.5943 Validation Accuracy: 62.259990\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.5929 Validation Accuracy: 60.019994\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.7251 Validation Accuracy: 61.799991\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.6176 Validation Accuracy: 61.639988\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.5534 Validation Accuracy: 62.619990\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.5991 Validation Accuracy: 61.899990\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.5844 Validation Accuracy: 60.559988\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.7141 Validation Accuracy: 61.919987\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.5954 Validation Accuracy: 62.219989\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.5621 Validation Accuracy: 62.599993\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.5796 Validation Accuracy: 62.439990\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.5701 Validation Accuracy: 60.459989\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.7327 Validation Accuracy: 61.559987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.6106 Validation Accuracy: 61.919987\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.5583 Validation Accuracy: 61.879992\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.6000 Validation Accuracy: 61.279988\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.5928 Validation Accuracy: 59.559995\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.7214 Validation Accuracy: 60.879993\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.6115 Validation Accuracy: 61.619991\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.5660 Validation Accuracy: 61.939991\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.5868 Validation Accuracy: 61.839986\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.5520 Validation Accuracy: 61.559993\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.7386 Validation Accuracy: 61.059988\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.6055 Validation Accuracy: 61.599988\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.5742 Validation Accuracy: 61.839992\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.5925 Validation Accuracy: 61.459988\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.5567 Validation Accuracy: 60.419995\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.7215 Validation Accuracy: 60.099995\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.6233 Validation Accuracy: 61.679995\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.5589 Validation Accuracy: 62.479991\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.5787 Validation Accuracy: 62.099993\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.5300 Validation Accuracy: 61.139995\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.6869 Validation Accuracy: 61.299992\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.6047 Validation Accuracy: 61.739993\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.5523 Validation Accuracy: 62.199986\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.5877 Validation Accuracy: 61.919993\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.5241 Validation Accuracy: 60.739994\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.7369 Validation Accuracy: 60.299993\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.5960 Validation Accuracy: 61.539990\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.5253 Validation Accuracy: 62.379992\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.5781 Validation Accuracy: 62.439990\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.5504 Validation Accuracy: 59.799993\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.7018 Validation Accuracy: 60.439992\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.6061 Validation Accuracy: 61.779988\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.5584 Validation Accuracy: 61.799991\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.5781 Validation Accuracy: 61.939985\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.5496 Validation Accuracy: 60.499990\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.7329 Validation Accuracy: 59.079999\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.6114 Validation Accuracy: 61.739993\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.5545 Validation Accuracy: 62.319988\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.5713 Validation Accuracy: 62.539995\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.5271 Validation Accuracy: 60.579991\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.7153 Validation Accuracy: 60.219991\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.6073 Validation Accuracy: 61.159992\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.5682 Validation Accuracy: 61.879992\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.5706 Validation Accuracy: 62.179995\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.5362 Validation Accuracy: 60.879993\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.7294 Validation Accuracy: 59.160000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.5905 Validation Accuracy: 61.839986\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.5909 Validation Accuracy: 61.159998\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.5760 Validation Accuracy: 61.759984\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.5208 Validation Accuracy: 60.639989\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.7024 Validation Accuracy: 60.319996\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.6062 Validation Accuracy: 62.119997\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.6283 Validation Accuracy: 60.719991\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.5479 Validation Accuracy: 62.019992\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.5180 Validation Accuracy: 60.979992\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.7038 Validation Accuracy: 59.559995\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.5956 Validation Accuracy: 61.239994\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.5867 Validation Accuracy: 60.919994\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.5667 Validation Accuracy: 62.259984\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.5119 Validation Accuracy: 60.779989\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.7009 Validation Accuracy: 59.159994\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.5919 Validation Accuracy: 61.899996\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.5645 Validation Accuracy: 61.019993\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.5794 Validation Accuracy: 61.359990\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.5054 Validation Accuracy: 61.299992\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.6725 Validation Accuracy: 60.259992\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.5775 Validation Accuracy: 61.799991\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.5960 Validation Accuracy: 60.799992\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.5502 Validation Accuracy: 61.979985\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.5039 Validation Accuracy: 61.459994\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.6694 Validation Accuracy: 60.199994\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.5803 Validation Accuracy: 62.479997\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.5454 Validation Accuracy: 61.079991\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.5639 Validation Accuracy: 61.619985\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.5059 Validation Accuracy: 61.299992\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.6573 Validation Accuracy: 60.719985\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.5689 Validation Accuracy: 62.339997\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.5521 Validation Accuracy: 61.639994\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.5471 Validation Accuracy: 61.159992\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.4990 Validation Accuracy: 61.199993\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.6747 Validation Accuracy: 60.239995\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.5743 Validation Accuracy: 62.099993\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.5650 Validation Accuracy: 60.939991\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.5619 Validation Accuracy: 61.499989\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.5189 Validation Accuracy: 61.579984\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.6679 Validation Accuracy: 60.379988\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.5933 Validation Accuracy: 61.739987\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.5251 Validation Accuracy: 61.839986\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.5376 Validation Accuracy: 62.259990\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.4879 Validation Accuracy: 62.279987\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.6512 Validation Accuracy: 60.699987\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.5886 Validation Accuracy: 61.739987\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.5392 Validation Accuracy: 61.259991\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.5373 Validation Accuracy: 62.399995\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.5117 Validation Accuracy: 61.959994\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.6850 Validation Accuracy: 60.299987\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.5755 Validation Accuracy: 62.319988\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.5154 Validation Accuracy: 62.179995\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.5334 Validation Accuracy: 62.459993\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.4988 Validation Accuracy: 61.759990\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.6513 Validation Accuracy: 60.159993\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.5451 Validation Accuracy: 61.579990\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.5087 Validation Accuracy: 61.839986\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.5246 Validation Accuracy: 62.279987\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.5029 Validation Accuracy: 61.239994\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.6444 Validation Accuracy: 60.139990\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.5508 Validation Accuracy: 62.619990\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.5557 Validation Accuracy: 60.679996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.5286 Validation Accuracy: 62.459987\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.4614 Validation Accuracy: 62.299991\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.6178 Validation Accuracy: 60.919988\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.5343 Validation Accuracy: 62.379992\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.5198 Validation Accuracy: 61.179990\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.5305 Validation Accuracy: 61.919987\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.4823 Validation Accuracy: 61.279994\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.6384 Validation Accuracy: 60.619992\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.5367 Validation Accuracy: 62.379992\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.5296 Validation Accuracy: 60.939991\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.5119 Validation Accuracy: 62.039995\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.4623 Validation Accuracy: 61.759984\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.6231 Validation Accuracy: 60.639995\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.5138 Validation Accuracy: 62.159991\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.4878 Validation Accuracy: 61.339992\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.5161 Validation Accuracy: 61.619991\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.4432 Validation Accuracy: 62.119991\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.6578 Validation Accuracy: 60.279995\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.5320 Validation Accuracy: 62.079996\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.5108 Validation Accuracy: 61.259991\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.5193 Validation Accuracy: 62.439984\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.4441 Validation Accuracy: 61.539996\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.6385 Validation Accuracy: 60.199988\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.5272 Validation Accuracy: 61.679989\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.4629 Validation Accuracy: 62.579989\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.5149 Validation Accuracy: 61.699986\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.4476 Validation Accuracy: 61.579990\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.6393 Validation Accuracy: 59.779990\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.5276 Validation Accuracy: 61.719990\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.4559 Validation Accuracy: 62.599987\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.5071 Validation Accuracy: 61.859989\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.4568 Validation Accuracy: 61.179996\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.6414 Validation Accuracy: 60.179985\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.5513 Validation Accuracy: 60.739988\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.4608 Validation Accuracy: 62.759995\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.5145 Validation Accuracy: 62.139988\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.4464 Validation Accuracy: 61.559987\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.6203 Validation Accuracy: 60.979992\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.5411 Validation Accuracy: 60.739994\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.4703 Validation Accuracy: 62.019992\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.4926 Validation Accuracy: 62.359989\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.4309 Validation Accuracy: 62.139988\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.6201 Validation Accuracy: 60.599989\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.5553 Validation Accuracy: 59.579986\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.5109 Validation Accuracy: 61.059988\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.5551 Validation Accuracy: 60.759997\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.4548 Validation Accuracy: 61.279994\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.6226 Validation Accuracy: 60.799998\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.5856 Validation Accuracy: 60.219991\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.4948 Validation Accuracy: 61.639994\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.5569 Validation Accuracy: 60.439992\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.4749 Validation Accuracy: 60.899991\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.6098 Validation Accuracy: 61.159992\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.5508 Validation Accuracy: 60.179996\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.4913 Validation Accuracy: 61.119992\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.5417 Validation Accuracy: 60.499990\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.4591 Validation Accuracy: 61.059994\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.5982 Validation Accuracy: 61.679995\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.5357 Validation Accuracy: 61.259991\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.4600 Validation Accuracy: 62.479991\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.5331 Validation Accuracy: 60.719997\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.4495 Validation Accuracy: 61.239988\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.5841 Validation Accuracy: 61.559993\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.5246 Validation Accuracy: 61.239988\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.4787 Validation Accuracy: 61.759990\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.5323 Validation Accuracy: 60.979992\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.4636 Validation Accuracy: 60.539997\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.5777 Validation Accuracy: 61.979985\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.5243 Validation Accuracy: 60.979998\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.4691 Validation Accuracy: 61.719996\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.5689 Validation Accuracy: 59.479988\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.4377 Validation Accuracy: 61.359990\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.5848 Validation Accuracy: 62.559992\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.5452 Validation Accuracy: 60.719991\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.4693 Validation Accuracy: 61.519986\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.5562 Validation Accuracy: 60.179991\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.4535 Validation Accuracy: 61.639988\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.6482 Validation Accuracy: 60.879993\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.5200 Validation Accuracy: 61.259991\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.5037 Validation Accuracy: 61.159992\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.5549 Validation Accuracy: 59.759992\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.4395 Validation Accuracy: 62.219989\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.6061 Validation Accuracy: 61.459988\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.5202 Validation Accuracy: 61.239994\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.5013 Validation Accuracy: 60.999995\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.5362 Validation Accuracy: 60.219991\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.4444 Validation Accuracy: 62.399995\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.6018 Validation Accuracy: 61.559987\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.4932 Validation Accuracy: 61.479992\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.5041 Validation Accuracy: 61.159986\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.5165 Validation Accuracy: 60.339993\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.4389 Validation Accuracy: 62.579995\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.5920 Validation Accuracy: 61.359990\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.4871 Validation Accuracy: 61.159998\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.4981 Validation Accuracy: 61.039990\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.5226 Validation Accuracy: 60.459995\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.4264 Validation Accuracy: 62.219989\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.5856 Validation Accuracy: 61.179996\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.5082 Validation Accuracy: 61.119986\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.4875 Validation Accuracy: 61.199993\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.5221 Validation Accuracy: 59.919989\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.4361 Validation Accuracy: 62.379992\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.6286 Validation Accuracy: 61.159992\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.5036 Validation Accuracy: 60.599989\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.5010 Validation Accuracy: 61.639988\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.5715 Validation Accuracy: 59.579986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.4583 Validation Accuracy: 61.699992\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.5796 Validation Accuracy: 61.459988\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.4862 Validation Accuracy: 61.719990\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.4739 Validation Accuracy: 61.599994\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.5491 Validation Accuracy: 59.339994\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.4631 Validation Accuracy: 61.319983\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.5882 Validation Accuracy: 61.559987\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.4794 Validation Accuracy: 61.739987\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.4481 Validation Accuracy: 61.559993\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.5549 Validation Accuracy: 59.599996\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.4506 Validation Accuracy: 61.459988\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.5842 Validation Accuracy: 60.679996\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.4771 Validation Accuracy: 61.719990\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.4805 Validation Accuracy: 60.859990\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.5602 Validation Accuracy: 59.919989\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.4470 Validation Accuracy: 61.799991\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.5912 Validation Accuracy: 61.439991\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.4946 Validation Accuracy: 61.359996\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.4946 Validation Accuracy: 60.919994\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.5114 Validation Accuracy: 60.959995\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.4249 Validation Accuracy: 61.719990\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.5897 Validation Accuracy: 61.219990\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.4589 Validation Accuracy: 61.939996\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.4585 Validation Accuracy: 61.699992\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.5000 Validation Accuracy: 60.519993\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.4219 Validation Accuracy: 61.739993\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.5621 Validation Accuracy: 61.619997\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.4438 Validation Accuracy: 61.859989\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.4580 Validation Accuracy: 61.339986\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.5037 Validation Accuracy: 60.779989\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.4050 Validation Accuracy: 62.419993\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.5684 Validation Accuracy: 61.299992\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.4368 Validation Accuracy: 60.879993\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.4559 Validation Accuracy: 61.179990\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.5239 Validation Accuracy: 59.799993\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.4290 Validation Accuracy: 61.499989\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.5557 Validation Accuracy: 60.979986\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.4267 Validation Accuracy: 61.919987\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.4269 Validation Accuracy: 61.279988\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.4977 Validation Accuracy: 60.679996\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.4132 Validation Accuracy: 61.879992\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.5578 Validation Accuracy: 61.259997\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.4567 Validation Accuracy: 61.139989\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.4467 Validation Accuracy: 61.119992\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.5187 Validation Accuracy: 59.959990\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.4284 Validation Accuracy: 61.759990\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.5888 Validation Accuracy: 60.459989\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.4627 Validation Accuracy: 60.859990\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.4282 Validation Accuracy: 61.899984\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.5093 Validation Accuracy: 60.559988\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.4243 Validation Accuracy: 61.459988\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.6320 Validation Accuracy: 59.779996\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.4592 Validation Accuracy: 61.519992\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.4682 Validation Accuracy: 61.219990\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.5199 Validation Accuracy: 60.399991\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.4290 Validation Accuracy: 61.359990\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.6083 Validation Accuracy: 60.579991\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.4601 Validation Accuracy: 61.579990\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.4447 Validation Accuracy: 61.479986\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.5055 Validation Accuracy: 60.659987\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.4248 Validation Accuracy: 61.139989\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.5881 Validation Accuracy: 61.159992\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.4604 Validation Accuracy: 61.259985\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.4123 Validation Accuracy: 62.319988\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.4890 Validation Accuracy: 61.099994\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.4036 Validation Accuracy: 61.779988\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.5895 Validation Accuracy: 61.199993\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.4451 Validation Accuracy: 61.399990\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.4120 Validation Accuracy: 61.879992\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.4841 Validation Accuracy: 61.299992\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.4383 Validation Accuracy: 61.219990\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.5745 Validation Accuracy: 61.299992\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.4268 Validation Accuracy: 61.419988\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.4040 Validation Accuracy: 62.019992\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.4702 Validation Accuracy: 61.479992\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.4488 Validation Accuracy: 60.239995\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.5710 Validation Accuracy: 60.979986\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.4511 Validation Accuracy: 61.419994\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.4010 Validation Accuracy: 61.239994\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.4700 Validation Accuracy: 61.859989\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.4476 Validation Accuracy: 60.419989\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.5430 Validation Accuracy: 61.639988\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.4423 Validation Accuracy: 61.279994\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.4078 Validation Accuracy: 61.379993\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.4707 Validation Accuracy: 61.139989\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.4356 Validation Accuracy: 60.579991\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.5622 Validation Accuracy: 60.959989\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.4313 Validation Accuracy: 61.339986\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.3896 Validation Accuracy: 61.659986\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.4813 Validation Accuracy: 61.059988\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.4116 Validation Accuracy: 60.119992\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.5346 Validation Accuracy: 60.879987\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.4199 Validation Accuracy: 61.899990\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.3956 Validation Accuracy: 61.619991\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.4814 Validation Accuracy: 60.479987\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.4169 Validation Accuracy: 60.079992\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.5954 Validation Accuracy: 60.539991\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.4310 Validation Accuracy: 61.659992\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.4256 Validation Accuracy: 60.539991\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.4756 Validation Accuracy: 61.239994\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.4446 Validation Accuracy: 59.959996\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.5799 Validation Accuracy: 60.899997\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.4488 Validation Accuracy: 61.399990\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.4089 Validation Accuracy: 60.159993\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.4667 Validation Accuracy: 60.579991\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.3996 Validation Accuracy: 61.059988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.5765 Validation Accuracy: 60.679996\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.4428 Validation Accuracy: 61.499989\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.3988 Validation Accuracy: 60.559988\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     0.4754 Validation Accuracy: 60.719991\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.4011 Validation Accuracy: 60.359991\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.5609 Validation Accuracy: 61.139989\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.4250 Validation Accuracy: 60.959995\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.4179 Validation Accuracy: 60.679990\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     0.4788 Validation Accuracy: 61.039996\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.3983 Validation Accuracy: 60.199988\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.5475 Validation Accuracy: 60.679990\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.4180 Validation Accuracy: 61.919987\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.4136 Validation Accuracy: 60.939991\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     0.4797 Validation Accuracy: 61.059994\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.3942 Validation Accuracy: 60.619992\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.5502 Validation Accuracy: 60.759991\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.4194 Validation Accuracy: 61.699986\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.4266 Validation Accuracy: 60.459995\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     0.4647 Validation Accuracy: 60.799992\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.4218 Validation Accuracy: 60.059988\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.5632 Validation Accuracy: 60.919994\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.4189 Validation Accuracy: 61.359990\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.3909 Validation Accuracy: 60.239995\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     0.4704 Validation Accuracy: 60.399991\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.3938 Validation Accuracy: 61.119992\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.5408 Validation Accuracy: 61.099988\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.4365 Validation Accuracy: 60.839987\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.4046 Validation Accuracy: 60.259992\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     0.4429 Validation Accuracy: 60.799992\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.3620 Validation Accuracy: 60.699993\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.5425 Validation Accuracy: 61.499989\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.4130 Validation Accuracy: 61.359990\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.3798 Validation Accuracy: 61.399990\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     0.4472 Validation Accuracy: 60.559994\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.3623 Validation Accuracy: 61.139989\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.5189 Validation Accuracy: 61.259991\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.4246 Validation Accuracy: 61.219990\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.3652 Validation Accuracy: 61.559987\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     0.4756 Validation Accuracy: 59.759992\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.3612 Validation Accuracy: 61.179996\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.5288 Validation Accuracy: 61.399996\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.4267 Validation Accuracy: 60.919988\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.3579 Validation Accuracy: 61.219996\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     0.4688 Validation Accuracy: 60.359991\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.3880 Validation Accuracy: 60.559994\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.5240 Validation Accuracy: 61.179990\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.4107 Validation Accuracy: 61.319995\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.3520 Validation Accuracy: 61.819983\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     0.4723 Validation Accuracy: 59.779990\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.3935 Validation Accuracy: 61.439991\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.5318 Validation Accuracy: 61.639994\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.4331 Validation Accuracy: 61.279994\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.3550 Validation Accuracy: 61.419988\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     0.4721 Validation Accuracy: 59.699994\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.3847 Validation Accuracy: 60.899991\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.5412 Validation Accuracy: 60.879993\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.4291 Validation Accuracy: 61.179990\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.3934 Validation Accuracy: 60.679996\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     0.4484 Validation Accuracy: 60.079992\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.4095 Validation Accuracy: 60.679990\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.5202 Validation Accuracy: 61.619997\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.4193 Validation Accuracy: 60.919988\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.4013 Validation Accuracy: 60.819989\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     0.4151 Validation Accuracy: 60.719991\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.3942 Validation Accuracy: 60.659993\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.5062 Validation Accuracy: 61.739987\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.4093 Validation Accuracy: 61.659998\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.4283 Validation Accuracy: 60.319990\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     0.4307 Validation Accuracy: 60.859990\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.4078 Validation Accuracy: 60.539991\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.5260 Validation Accuracy: 61.819994\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.4344 Validation Accuracy: 61.359996\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.4038 Validation Accuracy: 60.779995\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     0.4448 Validation Accuracy: 60.439986\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.3936 Validation Accuracy: 60.719997\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.5527 Validation Accuracy: 60.939986\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.4659 Validation Accuracy: 60.719991\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.4406 Validation Accuracy: 59.719986\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     0.5042 Validation Accuracy: 59.879994\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.3990 Validation Accuracy: 61.479986\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.5398 Validation Accuracy: 61.719990\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.4382 Validation Accuracy: 61.279988\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.4441 Validation Accuracy: 59.939992\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     0.4809 Validation Accuracy: 60.319990\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.4099 Validation Accuracy: 61.379999\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.5512 Validation Accuracy: 61.639988\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.4298 Validation Accuracy: 61.379993\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.3992 Validation Accuracy: 60.059994\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     0.4518 Validation Accuracy: 60.519993\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.3702 Validation Accuracy: 61.639994\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.5210 Validation Accuracy: 60.919988\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.4439 Validation Accuracy: 61.279988\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.3908 Validation Accuracy: 61.379987\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     0.4473 Validation Accuracy: 60.019994\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.3902 Validation Accuracy: 61.399984\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.5282 Validation Accuracy: 61.179990\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.4424 Validation Accuracy: 60.939991\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.3969 Validation Accuracy: 61.079991\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     0.4450 Validation Accuracy: 59.679985\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.3955 Validation Accuracy: 61.119986\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.5213 Validation Accuracy: 61.299986\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.4530 Validation Accuracy: 60.259992\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.3970 Validation Accuracy: 60.519987\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     0.4274 Validation Accuracy: 59.759992\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.3918 Validation Accuracy: 61.179996\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.5132 Validation Accuracy: 61.339998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.4395 Validation Accuracy: 61.639988\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.3648 Validation Accuracy: 60.919988\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     0.4373 Validation Accuracy: 60.039991\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.3952 Validation Accuracy: 60.939991\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.5040 Validation Accuracy: 61.119998\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.4318 Validation Accuracy: 61.079991\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.3661 Validation Accuracy: 61.139995\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     0.4493 Validation Accuracy: 60.059988\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.4099 Validation Accuracy: 60.999990\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.5072 Validation Accuracy: 61.719990\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.4381 Validation Accuracy: 60.779989\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.3660 Validation Accuracy: 61.039984\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     0.4600 Validation Accuracy: 59.899992\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.4015 Validation Accuracy: 61.299992\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.4904 Validation Accuracy: 61.539990\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.4034 Validation Accuracy: 61.639988\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.3424 Validation Accuracy: 61.879992\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     0.4399 Validation Accuracy: 59.959996\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.3919 Validation Accuracy: 61.159992\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.4983 Validation Accuracy: 61.719990\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.4088 Validation Accuracy: 61.219990\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.3580 Validation Accuracy: 61.579984\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     0.4486 Validation Accuracy: 60.259992\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.3721 Validation Accuracy: 61.219990\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.4899 Validation Accuracy: 61.619997\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.4100 Validation Accuracy: 61.819983\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.3748 Validation Accuracy: 61.079991\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     0.4346 Validation Accuracy: 60.179996\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.3716 Validation Accuracy: 61.319989\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.4817 Validation Accuracy: 61.379987\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.4146 Validation Accuracy: 62.059987\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.3446 Validation Accuracy: 61.679989\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     0.4512 Validation Accuracy: 60.479987\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.3968 Validation Accuracy: 61.079991\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.4947 Validation Accuracy: 60.899985\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.4050 Validation Accuracy: 61.739987\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.3689 Validation Accuracy: 61.099994\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     0.4727 Validation Accuracy: 59.379995\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.3853 Validation Accuracy: 61.739987\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.4513 Validation Accuracy: 62.179995\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.4085 Validation Accuracy: 60.799992\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.3568 Validation Accuracy: 61.519986\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     0.4605 Validation Accuracy: 59.659994\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.3640 Validation Accuracy: 61.919987\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.4707 Validation Accuracy: 61.399996\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.4141 Validation Accuracy: 60.959989\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.3825 Validation Accuracy: 60.839987\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     0.4393 Validation Accuracy: 59.839994\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.3725 Validation Accuracy: 61.339992\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.4800 Validation Accuracy: 61.899990\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.4115 Validation Accuracy: 61.519986\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.3473 Validation Accuracy: 61.039990\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     0.4589 Validation Accuracy: 59.299988\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.3633 Validation Accuracy: 61.579990\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.4440 Validation Accuracy: 62.319994\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.4183 Validation Accuracy: 61.259991\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.3449 Validation Accuracy: 61.359984\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     0.4530 Validation Accuracy: 58.719993\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.3598 Validation Accuracy: 61.999989\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.4360 Validation Accuracy: 62.559986\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.4093 Validation Accuracy: 60.839993\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.3536 Validation Accuracy: 61.299992\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     0.4223 Validation Accuracy: 60.259998\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.4018 Validation Accuracy: 60.539997\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.4854 Validation Accuracy: 61.059994\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.4277 Validation Accuracy: 61.179990\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.3499 Validation Accuracy: 61.239988\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     0.4073 Validation Accuracy: 59.659994\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.3833 Validation Accuracy: 60.919988\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.4703 Validation Accuracy: 61.419994\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.4147 Validation Accuracy: 61.739987\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.3421 Validation Accuracy: 60.959989\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     0.3997 Validation Accuracy: 59.779990\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.3583 Validation Accuracy: 61.119986\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.4678 Validation Accuracy: 61.859989\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.3969 Validation Accuracy: 61.259991\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.3211 Validation Accuracy: 61.319995\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     0.3883 Validation Accuracy: 59.819990\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.3610 Validation Accuracy: 61.779988\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.4620 Validation Accuracy: 61.319989\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.4079 Validation Accuracy: 61.219990\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.3393 Validation Accuracy: 60.579991\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     0.3874 Validation Accuracy: 60.679990\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.3350 Validation Accuracy: 61.879992\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.4683 Validation Accuracy: 61.339992\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.4043 Validation Accuracy: 60.839987\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.3230 Validation Accuracy: 61.099994\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     0.3922 Validation Accuracy: 60.319996\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.3915 Validation Accuracy: 60.419995\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.5158 Validation Accuracy: 60.339993\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.4158 Validation Accuracy: 61.259991\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.3278 Validation Accuracy: 60.999990\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     0.3828 Validation Accuracy: 60.259992\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.3574 Validation Accuracy: 60.999990\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.4710 Validation Accuracy: 60.979992\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.3710 Validation Accuracy: 61.379993\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.3174 Validation Accuracy: 61.239988\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     0.3972 Validation Accuracy: 59.859991\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.3423 Validation Accuracy: 61.119992\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.4598 Validation Accuracy: 60.659987\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.3812 Validation Accuracy: 61.379993\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.3185 Validation Accuracy: 60.979992\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     0.3906 Validation Accuracy: 60.179991\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.3408 Validation Accuracy: 61.179990\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.4742 Validation Accuracy: 60.339993\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.3845 Validation Accuracy: 61.399990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.3114 Validation Accuracy: 61.479998\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     0.3993 Validation Accuracy: 60.059994\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.3456 Validation Accuracy: 61.139989\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.4803 Validation Accuracy: 60.739994\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.3936 Validation Accuracy: 60.999990\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.3212 Validation Accuracy: 61.439985\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     0.4082 Validation Accuracy: 58.739996\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.3557 Validation Accuracy: 60.079992\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.4627 Validation Accuracy: 60.699993\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.4014 Validation Accuracy: 61.119992\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.3117 Validation Accuracy: 60.759991\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     0.4319 Validation Accuracy: 59.219992\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.3531 Validation Accuracy: 61.259985\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.4630 Validation Accuracy: 60.719991\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.4026 Validation Accuracy: 61.019987\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.3327 Validation Accuracy: 60.599995\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     0.4107 Validation Accuracy: 59.539992\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.3566 Validation Accuracy: 61.039990\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.4734 Validation Accuracy: 60.659993\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.4033 Validation Accuracy: 61.379993\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.3179 Validation Accuracy: 60.959983\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     0.4050 Validation Accuracy: 60.159987\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.3485 Validation Accuracy: 61.019993\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.4623 Validation Accuracy: 61.579990\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.4152 Validation Accuracy: 61.179990\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.3469 Validation Accuracy: 61.059994\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     0.3996 Validation Accuracy: 59.899992\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.3270 Validation Accuracy: 61.259985\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.4525 Validation Accuracy: 61.879992\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.4170 Validation Accuracy: 61.199987\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.3288 Validation Accuracy: 61.119986\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     0.4038 Validation Accuracy: 60.359997\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.3376 Validation Accuracy: 60.679990\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.4306 Validation Accuracy: 62.139994\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.4035 Validation Accuracy: 61.239994\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.3364 Validation Accuracy: 60.619992\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     0.4343 Validation Accuracy: 59.199989\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.3322 Validation Accuracy: 60.599995\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.4482 Validation Accuracy: 62.259984\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.4293 Validation Accuracy: 60.619992\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.3219 Validation Accuracy: 60.499990\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     0.4258 Validation Accuracy: 59.499991\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.3515 Validation Accuracy: 60.459995\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.4489 Validation Accuracy: 61.879987\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.4021 Validation Accuracy: 61.599994\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.3226 Validation Accuracy: 61.319995\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     0.4089 Validation Accuracy: 60.099989\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.3542 Validation Accuracy: 60.379988\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.4641 Validation Accuracy: 61.639988\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.4047 Validation Accuracy: 61.119992\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.3270 Validation Accuracy: 60.459989\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     0.3881 Validation Accuracy: 59.619999\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.3374 Validation Accuracy: 60.419983\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.4631 Validation Accuracy: 61.599988\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.3876 Validation Accuracy: 60.539997\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.2998 Validation Accuracy: 61.519992\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     0.3819 Validation Accuracy: 60.959989\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.3352 Validation Accuracy: 60.739994\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.4644 Validation Accuracy: 61.499989\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.3733 Validation Accuracy: 61.419988\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.2970 Validation Accuracy: 61.499989\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     0.3782 Validation Accuracy: 60.639995\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.3252 Validation Accuracy: 60.519993\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.4605 Validation Accuracy: 61.099994\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.3683 Validation Accuracy: 61.139989\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.2902 Validation Accuracy: 61.379987\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     0.3904 Validation Accuracy: 60.239995\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.3249 Validation Accuracy: 60.579991\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.4549 Validation Accuracy: 61.619997\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.3781 Validation Accuracy: 61.079991\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.2795 Validation Accuracy: 61.419994\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     0.3721 Validation Accuracy: 60.439992\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.3020 Validation Accuracy: 60.459989\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.4526 Validation Accuracy: 61.079991\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.3722 Validation Accuracy: 61.419988\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.2835 Validation Accuracy: 61.479992\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     0.3587 Validation Accuracy: 60.359991\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.3069 Validation Accuracy: 61.099988\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.4598 Validation Accuracy: 61.519992\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.3560 Validation Accuracy: 61.499995\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.2617 Validation Accuracy: 61.799991\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     0.3659 Validation Accuracy: 60.459989\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.3052 Validation Accuracy: 61.059994\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.4426 Validation Accuracy: 61.259991\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.3610 Validation Accuracy: 61.099994\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.2782 Validation Accuracy: 61.459994\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     0.3632 Validation Accuracy: 60.699993\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.3045 Validation Accuracy: 61.299986\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.4362 Validation Accuracy: 61.119986\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.3608 Validation Accuracy: 61.059988\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.3031 Validation Accuracy: 61.119998\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     0.3890 Validation Accuracy: 59.939992\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.2870 Validation Accuracy: 61.339986\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.4296 Validation Accuracy: 61.379987\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.3573 Validation Accuracy: 60.939997\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.2618 Validation Accuracy: 61.759990\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     0.3618 Validation Accuracy: 59.819996\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.2936 Validation Accuracy: 60.659993\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.4172 Validation Accuracy: 61.499995\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.3447 Validation Accuracy: 61.359990\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.2838 Validation Accuracy: 61.199987\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     0.3549 Validation Accuracy: 60.099995\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.2843 Validation Accuracy: 61.159986\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.4104 Validation Accuracy: 61.679983\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.3573 Validation Accuracy: 60.799986\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.2871 Validation Accuracy: 60.959989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     0.3368 Validation Accuracy: 60.719991\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.3002 Validation Accuracy: 60.979986\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.4162 Validation Accuracy: 61.539990\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.3559 Validation Accuracy: 61.419994\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.2657 Validation Accuracy: 61.759990\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     0.3566 Validation Accuracy: 61.019993\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.3020 Validation Accuracy: 61.219990\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.4268 Validation Accuracy: 61.019987\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.3502 Validation Accuracy: 61.279988\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.2870 Validation Accuracy: 60.659993\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     0.3693 Validation Accuracy: 60.139990\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.2954 Validation Accuracy: 60.799992\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.3946 Validation Accuracy: 61.139989\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.3925 Validation Accuracy: 59.779996\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.3133 Validation Accuracy: 60.279989\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     0.3606 Validation Accuracy: 60.119987\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.3051 Validation Accuracy: 61.099988\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.4025 Validation Accuracy: 61.219990\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.3574 Validation Accuracy: 60.439992\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.2801 Validation Accuracy: 61.319995\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     0.3686 Validation Accuracy: 60.679996\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.3290 Validation Accuracy: 60.479993\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.4129 Validation Accuracy: 61.259991\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.3778 Validation Accuracy: 60.539997\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.2676 Validation Accuracy: 61.179996\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     0.3300 Validation Accuracy: 60.779995\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.2954 Validation Accuracy: 61.479998\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.4082 Validation Accuracy: 60.579991\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.3884 Validation Accuracy: 60.279989\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.2843 Validation Accuracy: 61.459988\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     0.3533 Validation Accuracy: 60.479993\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.2840 Validation Accuracy: 61.659986\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.4455 Validation Accuracy: 60.099995\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.3767 Validation Accuracy: 60.839993\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.2657 Validation Accuracy: 61.879992\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.3829 Validation Accuracy: 60.259992\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.2923 Validation Accuracy: 61.379993\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.4249 Validation Accuracy: 60.699993\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.3403 Validation Accuracy: 60.959989\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.2971 Validation Accuracy: 61.359990\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.3767 Validation Accuracy: 59.919995\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.3001 Validation Accuracy: 61.339986\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session(config=config) as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6220358461141586\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njkVEREREov8PGivXiEhp9ywAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4634cfec50>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
